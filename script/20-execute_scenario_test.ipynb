{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c23e2e2-de60-41e1-a0e6-8d4918f7ec20",
   "metadata": {},
   "source": [
    "# From test prompts to LLM answers\n",
    "\n",
    "This notebook implements the execution of a scenario (prompts) designed in *stc-rdf-test*.\n",
    "\n",
    "Note : The code in the cells bellow should be added in a main function to be executed in command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b497d6c-f6f4-4eff-b975-5776768fb948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.2 (v3.13.2:4f8bb3947cf, Feb  4 2025, 11:51:10) [Clang 15.0.0 (clang-1500.3.9.4)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04ae821-e326-4921-9c6c-e3a95c050247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from llms_classes import *\n",
    "from prompt_classes import *\n",
    "from dataset_classes import TextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6b2e87-6a79-42d6-88e7-db2428ec790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=TextDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468c4922-3581-46ec-98fb-58123a4d5bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.filter_dataset_notclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a686d26d-6a6a-451f-9575-c2d34c5c3428",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=PromptCreation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07f58688-c59f-4f6b-8e25-4ad4dfdd97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=p.prompt_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e69a6ec-2ce0-4b6e-bcd2-0efdbdc41d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sei un classificatore di testi pubblicati su twitter (adesso X). Pensi che chi ha scritto la frase voglia attaccare, sostenere o essere neutrale verso la comunità LGBTQ+? Rispondi con un unica parola. \n",
      "\n",
      "INPUT: Meglio fascista che frocio? Sceglie fra due disgrazie. Sono contro l'omofobia, ma i froci non hanno mai mandato in Russia 67k mila ragazzi senza equipaggiamento, dei quali solo 7k tornati ma congelati, fra cui mio nonno. E' fiera di suo nonno che ha fatto questo  e .... vergogna!\n",
      "LABEL: Attacca\n",
      "\n",
      "INPUT: @Rosi33998582 @mps274 No. Questo è un insulto lesbica no LGBT sta per Lesbica Gay Bisessuali Trans usare la parola froc*o ha una connotazione dispregiativa come lo è diventata neg*o che invece prima si usava ( se sbaglio correggetemi per favore ).\n",
      "LABEL: Sostiene\n",
      "\n",
      "INPUT: {}\n",
      "LABEL: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in res:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb068308-ec18-4c3b-aa1e-8367fa49ffe6",
   "metadata": {},
   "source": [
    "## 2. Setup the Ollama server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a96293e-0521-412e-b8dd-178b0740e47e",
   "metadata": {},
   "source": [
    "Before executing this code, an Ollama server instance has to be launched in the terminal using the ```ollama serve``` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea7961d-72f5-450c-8de4-3df932d2cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama3.1\"\n",
    "MODEL1= \"mistral\"\n",
    "MODEL = \"qwen2.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea45c7a-e4b2-4c9d-a126-4e576e4c1cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models: ['qwen2.5:latest', 'deepseek-r1:7b', 'llama3.2:3b', 'gemma2:2b', 'gemma2:latest', 'phi3.5:latest', 'llama3.1:latest', 'qwen2:latest', 'phi3:latest', 'tinyllama:latest', 'qwen2:1.5b', 'qwen2:0.5b', 'llama3:latest']\n",
      "qwen2.5 is already available in the server\n"
     ]
    }
   ],
   "source": [
    "# Init the ollama server\n",
    "ollama_server = OllamaServer(ollama)\n",
    "\n",
    "# Check models that have already been downloaded\n",
    "models = ollama_server.get_models_list()\n",
    "print(\"Available Models:\", models)\n",
    "\n",
    "# Download the model to use for experiences\n",
    "models = ollama_server.download_model_if_not_exists(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a456df4-ffe8-4165-b70c-179e7d724e64",
   "metadata": {},
   "source": [
    "## 3. Execute the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60682202-c386-40b4-8d0c-ab1dafbe8495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen2.5 is already available in the server\n"
     ]
    }
   ],
   "source": [
    "# Initialize the client and server\n",
    "ollama_server = OllamaServer(ollama)\n",
    "\n",
    "# Initialize chat with a specific model\n",
    "chat = OllamaChat(server=ollama_server, model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7894695f-f490-432b-8343-0c7cba97572f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_filter\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTEXT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "d.dataset_filter['TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b99e2e5c-8559-4176-97b4-9885c4461a89",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m LLM_Responses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_filter\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTEXT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m################## PROMPT #####################\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(res\u001b[38;5;241m.\u001b[39mto_list()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(i))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "LLM_Responses = []\n",
    "\n",
    "for i in range(len(d.dataset_filter['TEXT'])):\n",
    "    \n",
    "    print(\"################## PROMPT #####################\")\n",
    "    print(res.to_list()[0].format(i))\n",
    "    print(\"################## RESPONSE #####################\")\n",
    "    print(\"Pred\")\n",
    "    response = chat.send_prompt(res.to_list()[0], prompt_uuid=\"1\", use_history=False, stream=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"True\")\n",
    "    print(d.dataset_filter['Pensi che chi ha scritto la frase voglia attaccare, sostenere o essere neutrale verso la comunità LGBTQ+?'][i])\n",
    "    print(\"\\n\")\n",
    "    #LLM_Responses.append(response)\n",
    "    #print(response.row_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46ea98-d0e8-4083-9af0-9e3d334d0824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ded90f-6889-4644-becf-be08d8e1a780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa13e98-7f53-40f0-988b-1cb1fb754a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c352a3-354b-49eb-ac43-436de80d5631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f9a80-6fc2-488e-b54f-f5ab69221b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
